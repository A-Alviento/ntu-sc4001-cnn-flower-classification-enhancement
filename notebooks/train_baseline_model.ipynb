{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "os.chdir(\"../models\")\n",
    "from model import CustomCNN\n",
    "from common_utils import set_seed, EarlyStopper, train_step, val_step, train, save_model, transform\n",
    "\n",
    "# set seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomCNN() # initialise model\n",
    "\n",
    "# load data\n",
    "train_dataset = datasets.Flowers102(root='../data', split='test', download=True, transform=transform) \n",
    "val_dataset = datasets.Flowers102(root='../data', split='val', download=True, transform=transform) \n",
    "test_dataset = datasets.Flowers102(root='../data', split='train', download=True, transform=transform)\n",
    "# NOTE: Due to a bug with the Flowers102 dataset, the train and test splits are swapped\n",
    "\n",
    "# initialise dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001 # learning rate\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr) # initialise optimiser\n",
    "loss = torch.nn.CrossEntropyLoss() # initialise loss function\n",
    "\n",
    "if torch.cuda.is_available(): # nvidia gpu\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): # apple gpu\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "epochs = 100 # number of epochs\n",
    "early_stopper = EarlyStopper(patience=10) # initialise early stopper\n",
    "\n",
    "\n",
    "# Make directory to save baseline model\n",
    "baseline_model_path = \"./baseline_model\"\n",
    "if not os.path.exists(baseline_model_path):\n",
    "    os.mkdir(baseline_model_path)\n",
    "\n",
    "# Define the device-specific path\n",
    "device_type = None\n",
    "if device == torch.device(\"cuda\"):\n",
    "    device_type = \"cuda\"\n",
    "elif device == torch.device(\"mps\"):\n",
    "    device_type = \"mps\"\n",
    "else:\n",
    "    device_type = \"cpu\"\n",
    "\n",
    "# Construct the full path\n",
    "device_path = os.path.join(baseline_model_path, device_type)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(device_path):\n",
    "    os.mkdir(device_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|          | 0/193 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrian/miniconda3/envs/sc4001/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Epoch 1/100: 100%|██████████| 193/193 [01:18<00:00,  2.47it/s, Training loss=4.4661]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 took 84.62s | Train loss: 4.4661 | Val loss: 4.6956 | Val accuracy: 0.98% | EarlyStopper count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:  32%|███▏      | 61/193 [00:25<00:55,  2.39it/s, Training loss=4.4232]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model, train_dataloader, val_dataloader, optimiser, loss, device, epochs, early_stopper, device_path) \u001b[39m# train model\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/sc4001-neural-networks-and-deep-learning/practical/project/sc4001-project/models/common_utils.py:118\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, tl, vl, opt, loss, device, epochs, early_stopper, path)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39m# Wrap the trainloader with tqdm for the progress bar\u001b[39;00m\n\u001b[1;32m    116\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(\u001b[39menumerate\u001b[39m(tl), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(tl), desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m train_loss \u001b[39m=\u001b[39m train_step(model, pbar, opt, device, loss)  \u001b[39m# Pass the tqdm-wrapped loader\u001b[39;00m\n\u001b[1;32m    119\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m val_step(model, vl, loss, device)\n\u001b[1;32m    121\u001b[0m train_loss_list\u001b[39m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/Desktop/sc4001-neural-networks-and-deep-learning/practical/project/sc4001-project/models/common_utils.py:68\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, trainloader, optimizer, device, lossfn)\u001b[0m\n\u001b[1;32m     65\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     66\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 68\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()  \u001b[39m# accumulate the loss\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     trainloader\u001b[39m.\u001b[39mset_postfix({\u001b[39m'\u001b[39m\u001b[39mTraining loss\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(total_loss\u001b[39m/\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m))})  \u001b[39m# Update the progress bar with the training loss\u001b[39;00m\n\u001b[1;32m     71\u001b[0m train_loss \u001b[39m=\u001b[39m total_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(trainloader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, val_dataloader, optimiser, loss, device, epochs, early_stopper, device_path) # train model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc4001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
